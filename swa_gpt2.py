# -*- coding: utf-8 -*-
"""Swa-GPT2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hXxcthOo1nMv5kp2HT_K-CjDitLKkM7l
"""


from datasets import load_dataset, get_dataset_split_names
from transformers import PreTrainedTokenizerFast
from tokenizers import Tokenizer

def load_huggingface_dataset(dataset_name,*args,**kwargs):
    dataset = load_dataset(dataset_name,**kwargs)
    return dataset

dataset = load_huggingface_dataset("nguthiru/swa_syllabic")

def generate_dataset_splits(dataset):
  # Split the dataset into train, test and val
  train_dataset = dataset["train"]
  test_dataset = dataset["test"]
  val_dataset = dataset["val"]
  return train_dataset,test_dataset,val_dataset

train_dataset, test_dataset, val_dataset = generate_dataset_splits(dataset)


class KiswahiliSilabiTokenizer(PreTrainedTokenizerFast):
    def __init__(self, tokenizer,unk_token="[UNK]",sos_token="[SOS]",eos_token="[EOS]",space_token="[SPACE]",pad_token="[PAD]", **kwargs):
        super().__init__(tokenizer_object=tokenizer, **kwargs)
        self._vocab = tokenizer.get_vocab()
        self.unk_token = unk_token
        self.sos_token = sos_token
        self.eos_token = eos_token
        self.space_token = space_token
        self.pad_token = pad_token

      # Add special tokens to vocab if they are not already present
        if self.sos_token not in self._vocab:
            self._vocab[self.sos_token] = len(self._vocab)
        if self.eos_token not in self._vocab:
            self._vocab[self.eos_token] = len(self._vocab)
        if self.unk_token not in self._vocab:
            self._vocab[self.unk_token] = len(self._vocab)
        if self.space_token not in self._vocab:
            self._vocab[self.space_token] = len(self._vocab)
        if self.pad_token not in self._vocab:
            self._vocab[self.pad_token] = len(self._vocab)

    def __call__(self, text,**kwargs):
        ids = self.convert_tokens_to_ids(self.tokenize(text,**kwargs))

        return {"input_ids": ids}

    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):
        tokenizer = Tokenizer.from_file(f"{pretrained_model_name_or_path}/tokenizer.json")
        return cls(tokenizer, **kwargs)

    def _encode_with_byte_fallback(self, text):
        tokens = []
        i = 0
        while i < len(text):
            matched = False
            # Try to match the longest syllable first
            for j in range(len(text), i, -1):
                syllable_candidate = text[i:j]
                if syllable_candidate in self._vocab:
                    tokens.append(syllable_candidate)
                    i = j
                    matched = True
                    break
            # If no syllable matched, fallback to byte encoding
            if not matched:
                if text[i] == " ":
                  tokens.append(self.space_token)
                  i += 1
                else:
                  tokens.extend(self.unk_token)
                  i += 1
        return tokens

    def tokenize(self, text,**kwargs):
        handle_whitespace = kwargs.get("handle_whitespace", True)
        tokens = [self.sos_token]  # Start of sentence token
        for word in text.split(" "):
            tokens.extend(self._encode_with_byte_fallback(word))
            if handle_whitespace:
              tokens.extend(self._encode_with_byte_fallback(" "))
        tokens.append(self.eos_token)  # End of sentence token

        padding = kwargs.get("padding", False)
        if padding:
            max_length = kwargs.get("max_length", None)
            if max_length is not None:
                tokens = tokens[:max_length]
                tokens.extend([self.pad_token] * (max_length - len(tokens)))
            else:
                raise ValueError("max_length must be specified if padding is True")
        return tokens

    def tokens_to_sentence(self,tokens):
      for token in tokens:
        token = token.replace(" ", "")
      sentence = "".join(tokens)
      sentence = sentence.replace(self.eos_token, "")
      sentence = sentence.replace(self.sos_token, "")
      sentence = sentence.replace(self.space_token," ")
      return sentence

silabi_tokenizer = KiswahiliSilabiTokenizer.from_pretrained("silabi_tokenizer")

train_dataset.set_format(type="torch", columns=["input_ids"])
val_dataset.set_format(type="torch", columns=["input_ids"])
test_dataset.set_format(type="torch", columns=["input_ids"])

from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(
    tokenizer=silabi_tokenizer,
    mlm=False,  # GPT-2 is not trained with masked language modeling
)

from transformers import GPT2Config, GPT2LMHeadModel

config = GPT2Config(
    vocab_size=silabi_tokenizer.vocab_size,
    n_positions=1024,
    n_ctx=1024,
    n_embd=768,
    n_layer=12,
    n_head=12,
    bos_token_id=silabi_tokenizer.bos_token_id,
    eos_token_id=silabi_tokenizer.eos_token_id,
)

gpt2_model = GPT2LMHeadModel(config)


from torchinfo import summary
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./swa_gpt2",
    evaluation_strategy="epoch",
    learning_rate=5e-4,
    weight_decay=0.01,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=4,
    num_train_epochs=6,
    save_strategy="epoch",
    save_total_limit=2,
    logging_dir="./logs",
    logging_steps=100,
)

trainer = Trainer(
    model=gpt2_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    processing_class=silabi_tokenizer,
    data_collator=data_collator,
)

trainer.train()


# Save the model
gpt2_model.save_pretrained("swa_gpt2")

# Push the model to the Hugging Face Hub
gpt2_model.push_to_hub("swa_gpt2")


# Evaluate the model
eval_results = trainer.evaluate()
print(f"Per Plexity: {torch.exp(torch.tensor(eval_results['eval_loss']))}")

# Use some examples to test the model
test_text = "Mimi ni mwanafunzi wa chuo kikuu"
input_ids = silabi_tokenizer(test_text)["input_ids"]
input_ids = torch.tensor(input_ids).unsqueeze(0)
output = gpt2_model.generate(input_ids, max_length=100, num_return_sequences=1)
output_text = silabi_tokenizer.tokens_to_sentence(silabi_tokenizer.decode(output[0].tolist()))

print(f"Input: {test_text}")
print(f"Output: {output_text}")


