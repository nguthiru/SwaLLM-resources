{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d3080-e795-4331-9979-68fa1e85fb78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67539aad-8e7e-403c-ad0d-fbd4eb8f0952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, get_dataset_split_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55948230-4ecb-4336-bf92-37504a55f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_huggingface_dataset(dataset_name,*args,**kwargs):\n",
    "    dataset = load_dataset(dataset_name,**kwargs)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1f11e43-c0a2-498f-bc40-6a80b1c1f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_huggingface_dataset(\"mwitiderrick/swahili\",split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9c2e0ad-eb77-4c04-9faf-9663f60f2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_splits(dataset):\n",
    "  # Split the dataset into train, test and val\n",
    "\n",
    "  train_dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "  test_val = train_dataset[\"test\"].train_test_split(\n",
    "      test_size=0.5, shuffle=True, seed=42\n",
    "  )\n",
    "  train_dataset = train_dataset[\"train\"]\n",
    "  test_dataset = test_val[\"test\"]\n",
    "  val_dataset = test_val[\"train\"]\n",
    "  return train_dataset,test_dataset,val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f19ccec1-97a7-455a-b376-a1a68efd0914",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, val_dataset = generate_dataset_splits(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81332eb8-c1ff-4c82-a145-46b6e32e1195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_non_text_symbols(text):\n",
    "\n",
    "  text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c7d4966-cdfe-4bec-a400-e41aab61cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(dataset):\n",
    "\n",
    "  # clean the dataset object\n",
    "  dataset = dataset.map(lambda example: {\"text\": remove_non_text_symbols(example[\"text\"])})\n",
    "  dataset = dataset.filter(lambda example: len(example[\"text\"]) > 0)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42fd994e-4612-4bcc-bfb3-19d1d97fe3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = clean_dataset(train_dataset)\n",
    "test_dataset = clean_dataset(test_dataset)\n",
    "val_dataset = clean_dataset(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "814c494f-62d4-438d-b44d-0fa6598f5c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f757edc-a6d8-464d-ad42-f7fc13126ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, dataset):\n",
    "  # tokenize the dataset\n",
    "  dataset = dataset.map(lambda example: tokenizer(example[\"text\"],padding=True,max_length=256))\n",
    "  return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97444a43-fbad-4e99-b947-19e2741f07a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KiswahiliSilabiTokenizer(PreTrainedTokenizerFast):\n",
    "    def __init__(self, tokenizer,unk_token=\"[UNK]\",sos_token=\"[SOS]\",eos_token=\"[EOS]\",space_token=\"[SPACE]\",pad_token=\"[PAD]\", **kwargs):\n",
    "        super().__init__(tokenizer_object=tokenizer, **kwargs)\n",
    "        self._vocab = tokenizer.get_vocab()\n",
    "        self.unk_token = unk_token\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.space_token = space_token\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "      # Add special tokens to vocab if they are not already present\n",
    "        if self.sos_token not in self._vocab:\n",
    "            self._vocab[self.sos_token] = len(self._vocab)\n",
    "        if self.eos_token not in self._vocab:\n",
    "            self._vocab[self.eos_token] = len(self._vocab)\n",
    "        if self.unk_token not in self._vocab:\n",
    "            self._vocab[self.unk_token] = len(self._vocab)\n",
    "        if self.space_token not in self._vocab:\n",
    "            self._vocab[self.space_token] = len(self._vocab)\n",
    "        if self.pad_token not in self._vocab:\n",
    "            self._vocab[self.pad_token] = len(self._vocab)\n",
    "\n",
    "    def __call__(self, text,**kwargs):\n",
    "        ids = self.convert_tokens_to_ids(self.tokenize(text,**kwargs))\n",
    "\n",
    "        return {\"input_ids\": ids}\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n",
    "        tokenizer = Tokenizer.from_file(f\"{pretrained_model_name_or_path}/tokenizer.json\")\n",
    "        return cls(tokenizer, **kwargs)\n",
    "\n",
    "    def _encode_with_byte_fallback(self, text):\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            matched = False\n",
    "            # Try to match the longest syllable first\n",
    "            for j in range(len(text), i, -1):\n",
    "                syllable_candidate = text[i:j]\n",
    "                if syllable_candidate in self._vocab:\n",
    "                    tokens.append(syllable_candidate)\n",
    "                    i = j\n",
    "                    matched = True\n",
    "                    break\n",
    "            # If no syllable matched, fallback to byte encoding\n",
    "            if not matched:\n",
    "                if text[i] == \" \":\n",
    "                  tokens.append(self.space_token)\n",
    "                  i += 1\n",
    "                else:\n",
    "                  tokens.extend(self.unk_token)\n",
    "                  i += 1\n",
    "        return tokens\n",
    "\n",
    "    def tokenize(self, text,**kwargs):\n",
    "        handle_whitespace = kwargs.get(\"handle_whitespace\", True)\n",
    "        tokens = [self.sos_token]  # Start of sentence token\n",
    "        for word in text.split(\" \"):\n",
    "            tokens.extend(self._encode_with_byte_fallback(word))\n",
    "            if handle_whitespace:\n",
    "              tokens.extend(self._encode_with_byte_fallback(\" \"))\n",
    "        tokens.append(self.eos_token)  # End of sentence token\n",
    "\n",
    "        padding = kwargs.get(\"padding\", False)\n",
    "        if padding:\n",
    "            max_length = kwargs.get(\"max_length\", None)\n",
    "            if max_length is not None:\n",
    "                tokens = tokens[:max_length]\n",
    "                tokens.extend([self.pad_token] * (max_length - len(tokens)))\n",
    "            else:\n",
    "                raise ValueError(\"max_length must be specified if padding is True\")\n",
    "        return tokens\n",
    "\n",
    "    def tokens_to_sentence(self,tokens):\n",
    "      for token in tokens:\n",
    "        token = token.replace(\" \", \"\")\n",
    "      sentence = \"\".join(tokens)\n",
    "      sentence = sentence.replace(self.eos_token, \"\")\n",
    "      sentence = sentence.replace(self.sos_token, \"\")\n",
    "      sentence = sentence.replace(self.space_token,\" \")\n",
    "      return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f189851f-1012-40b0-981b-c0788b1d9c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "silabi_tokenizer = KiswahiliSilabiTokenizer.from_pretrained(\"./silabi_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d0c11ef-4351-4784-a5ff-3f3895fc205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tokenized_dataset = tokenize(silabi_tokenizer, train_dataset)\n",
    "test_tokenized_dataset = tokenize(silabi_tokenizer, test_dataset)\n",
    "# val_tokenized_dataset = tokenize(silabi_tokenizer, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7fe393f4-4a2f-473c-b9e7-732d3657cb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09ea1ed401f44e184a5dd443f160345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/576513 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_tokenized_dataset = tokenize(silabi_tokenizer, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe34473f-5359-4699-b6f0-f4a6505e7153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fc11b41-c12b-4571-8c56-e2b987644460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.mamba.modeling_mamba import MambaBlock as Mamba\n",
    "from linear_attention_transformer import LinearAttentionTransformer\n",
    "from transformers.models.mamba import MambaConfig as MambaCfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31141879-e8e2-4552-82e1-58363690181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "\n",
    "test_value = test_tokenized_dataset[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a77fcfce-b607-413c-bbb3-a9865bc6acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(test_value)\n",
    "input_tensor = torch.from_numpy(np_array).to(dtype=torch.long)\n",
    "input_tensor = input_tensor.unsqueeze(0)  # Shape becomes (1, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699acb4e-e53b-4453-9353-8d77788fefb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f0f5545-92dc-407d-bc6b-b4ccebb925bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\n"
     ]
    }
   ],
   "source": [
    "mamba_config = MambaCfg(vocab_size=silabi_tokenizer.vocab_size,hidden_size=512,num_heads=16)\n",
    "# hidden_size = dimension\n",
    "mamba_block = Mamba(\n",
    "    config=mamba_config, layer_idx=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39d43e74-4b64-4cf2-852c-67cf9d4bf381",
   "metadata": {},
   "outputs": [],
   "source": [
    "mamba_embedding_dim = 512  # embedding dimension must match linformer dim\n",
    "vocab_size = silabi_tokenizer.vocab_size\n",
    "mamba_embedding_layer = torch.nn.Embedding(num_embeddings=silabi_tokenizer.vocab_size, embedding_dim=mamba_embedding_dim)\n",
    "mamba_embedded_tensor = mamba_embedding_layer(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68835ad2-edec-4003-8f25-c9f03e696bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 512])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mamba_embedded_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f38cdda-4932-4a97-a172-f7d2f00c6438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6630,  1.7532,  1.1520,  ..., -0.4268, -0.5768,  0.8503],\n",
       "         [ 0.2792, -0.2170,  0.4621,  ..., -0.9457, -0.3212,  0.6602],\n",
       "         [ 0.7157,  1.0502,  0.3434,  ...,  0.5062,  0.1723, -1.7375],\n",
       "         ...,\n",
       "         [-0.4800, -0.0839, -0.0720,  ...,  1.1306,  0.5015, -0.5833],\n",
       "         [-0.4800, -0.0839, -0.0720,  ...,  1.1306,  0.5015, -0.5833],\n",
       "         [-0.4800, -0.0839, -0.0720,  ...,  1.1306,  0.5015, -0.5833]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mamba_block(mamba_embedded_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e2327cb-55d4-48c4-b37d-18275c59028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch, sequence_length {256}) -> [embedding] -> (batch, sequence_length {256}. dimension {128}) -> [linformer] -> (batch, sequence_length {256}. dimension {128})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0e06041-14c8-4cbc-9812-b1a903d89e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "linformer = LinearAttentionTransformer(\n",
    "    dim = 256,\n",
    "    heads = 8,\n",
    "    depth = 1,\n",
    "    max_seq_len = 256,\n",
    "    n_local_attn_heads = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95524e78-2c81-44b1-b832-dbd62d1e7ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256  # embedding dimension must match linformer dim\n",
    "vocab_size = silabi_tokenizer.vocab_size\n",
    "embedding_layer = torch.nn.Embedding(num_embeddings=silabi_tokenizer.vocab_size, embedding_dim=embedding_dim)\n",
    "embedded_tensor = embedding_layer(input_tensor)  # Shape: (batch_size, sequence_length, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "448c9b85-4f06-47eb-a0a1-9be71bf30d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f8b1aca-9c62-4eaf-aca3-c7bde75b7965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4879,  2.1113,  1.7677,  ...,  0.9438, -2.0722,  0.2778],\n",
       "         [ 0.2190, -0.1404, -2.9204,  ...,  0.4524, -0.3791,  1.1368],\n",
       "         [ 0.0556,  0.0164, -0.3579,  ..., -0.1721, -0.0367,  1.7199],\n",
       "         ...,\n",
       "         [-0.7167,  1.1159, -1.6330,  ...,  0.3845, -1.8408, -2.7420],\n",
       "         [-0.7167,  1.1159, -1.6330,  ...,  0.3845, -1.8408, -2.7420],\n",
       "         [-0.7167,  1.1159, -1.6330,  ...,  0.3845, -1.8408, -2.7420]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linformer(embedded_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d489691-e415-40be-8140-a2719c0c1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LinformerConfig:\n",
    "    dim: int\n",
    "    heads: int\n",
    "    depth: int\n",
    "    max_seq_len: int\n",
    "    n_local_attn_heads: int\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"dim\": self.dim,\n",
    "            \"heads\": self.heads,\n",
    "            \"max_seq_len\": self.max_seq_len,\n",
    "            \"n_local_attn_heads\": self.n_local_attn_heads,\n",
    "            \"depth\": self.depth\n",
    "        }\n",
    "@dataclass\n",
    "class MambaConfig:\n",
    "    hidden_size: int # this is also the dimension\n",
    "    num_heads: int \n",
    "\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.hidden_size\n",
    "\n",
    "    def mamba2_config(self,vocab_size):\n",
    "        return MambaCfg(vocab_size=vocab_size,hidden_size=self.hidden_size,num_heads=self.num_heads)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45d6c17b-c7d0-43e4-94c1-6dc54b665c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self,vocab_size, config: MambaConfig,layer_idx=0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        #self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=config.dim)\n",
    "        self.normalization = nn.modules.normalization.RMSNorm(config.dim)\n",
    "        self.mamba2_block = Mamba(config = config.mamba2_config(self.vocab_size),layer_idx=layer_idx)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        #x = self.embedding(x)\n",
    "        x = self.normalization(x)\n",
    "        mamba_output = self.mamba2_block(x)\n",
    "        x = x + mamba_output\n",
    "        \n",
    "        x = self.normalization(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5851c3f0-d4a2-4c13-8e59-fbbb2269b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinformerBlock(nn.Module):\n",
    "    def __init__(self,vocab_size,config:LinformerConfig):\n",
    "        super().__init__()\n",
    "        #self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=config.dim)\n",
    "        self.normalization = nn.modules.normalization.RMSNorm(config.dim)\n",
    "        self.linformer = LinearAttentionTransformer(\n",
    "            **config.to_dict()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        #x = self.embedding(x)\n",
    "        x = self.normalization(x)\n",
    "        output = self.linformer(x)\n",
    "        x = output + x\n",
    "\n",
    "        x = self.normalization(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f95872d9-3852-4952-bbd2-9ad33f4ff357",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimbaBlock(nn.Module):\n",
    "    def __init__(self,linformer_config,mamba_config,layer_idx=0,vocab_size=1000,dropout = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mamba_block = MambaBlock(vocab_size,mamba_config,layer_idx=layer_idx)\n",
    "        self.linformer_block = LinformerBlock(vocab_size,linformer_config)\n",
    "        \n",
    "        self.linformer_mamba_reshape = nn.Linear(linformer_config.dim, mamba_config.dim)\n",
    "        self.mamba_linformer_reshape = nn.Linear(mamba_config.dim, linformer_config.dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # x -> (batch,seq_len)\n",
    "        x = self.mamba_block(x)\n",
    "\n",
    "        x = self.mamba_linformer_reshape(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linformer_block(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.linformer_mamba_reshape(x)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return x\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc7a263e-2a58-464f-af4e-55278fe94a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linformer_config = LinformerConfig(dim = 256,\n",
    "    heads = 8,\n",
    "    depth = 1,\n",
    "    max_seq_len = 256,\n",
    "    n_local_attn_heads = 4)\n",
    "mamba_config = MambaConfig(\n",
    "    hidden_size = 512,\n",
    "    num_heads = 16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed75f92f-0c65-4041-8226-be661205160c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990bde17-c772-4fdc-97fc-96343ccc5ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd388c0-5a06-4460-afb4-d529250e8445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7abd9cc-0ff9-45da-a562-2741ea5f2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Limba(nn.Module):\n",
    "    def __init__(self, linformer_config, mamba_config, vocab_size, num_layers=6,dropout=0.1):\n",
    "        super(Limba, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=mamba_config.dim)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                LimbaBlock(linformer_config, mamba_config, vocab_size) for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(mamba_config.dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "        self.output_layer = nn.Linear(mamba_config.dim, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass input through embedding layer\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        \n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e42aa3d-d263-41a7-85aa-fc81ba167e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Limba(linformer_config,mamba_config, silabi_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e516dce-4383-4f7a-b726-239b38b33d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0928, -0.1131,  0.2505,  ...,  0.6304, -0.7602,  0.4108],\n",
      "         [-0.1265,  0.2714, -0.9697,  ...,  0.3207,  0.4266,  0.1870],\n",
      "         [-0.2593, -0.7594,  0.5382,  ..., -0.1908,  0.2530, -0.6654],\n",
      "         ...,\n",
      "         [-0.5266, -0.4568, -0.0689,  ...,  1.3795, -0.2306,  0.8054],\n",
      "         [-0.5478, -0.1174,  0.5729,  ...,  1.2178, -0.1083,  0.1065],\n",
      "         [ 0.1767, -0.7181, -0.2846,  ...,  0.5680, -0.7983,  0.9244]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(input_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "676c988a-245d-4192-9b98-b38ee8d1860a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type:depth-idx)                                                      Output Shape              Param #\n",
       "=============================================================================================================================\n",
       "Limba                                                                       [1, 256, 662]             --\n",
       "├─Embedding: 1-1                                                            [1, 256, 512]             338,944\n",
       "├─ModuleList: 1-2                                                           --                        --\n",
       "│    └─LimbaBlock: 2-1                                                      [1, 256, 512]             --\n",
       "│    │    └─MambaBlock: 3-1                                                 [1, 256, 512]             1,695,744\n",
       "│    │    └─Linear: 3-2                                                     [1, 256, 256]             131,328\n",
       "│    │    └─Dropout: 3-3                                                    [1, 256, 256]             --\n",
       "│    │    └─LinformerBlock: 3-4                                             [1, 256, 256]             789,248\n",
       "│    │    └─Dropout: 3-5                                                    [1, 256, 256]             --\n",
       "│    │    └─Linear: 3-6                                                     [1, 256, 512]             131,584\n",
       "│    └─LimbaBlock: 2-2                                                      [1, 256, 512]             --\n",
       "│    │    └─MambaBlock: 3-7                                                 [1, 256, 512]             1,695,744\n",
       "│    │    └─Linear: 3-8                                                     [1, 256, 256]             131,328\n",
       "│    │    └─Dropout: 3-9                                                    [1, 256, 256]             --\n",
       "│    │    └─LinformerBlock: 3-10                                            [1, 256, 256]             789,248\n",
       "│    │    └─Dropout: 3-11                                                   [1, 256, 256]             --\n",
       "│    │    └─Linear: 3-12                                                    [1, 256, 512]             131,584\n",
       "│    └─LimbaBlock: 2-3                                                      [1, 256, 512]             --\n",
       "│    │    └─MambaBlock: 3-13                                                [1, 256, 512]             1,695,744\n",
       "│    │    └─Linear: 3-14                                                    [1, 256, 256]             131,328\n",
       "│    │    └─Dropout: 3-15                                                   [1, 256, 256]             --\n",
       "│    │    └─LinformerBlock: 3-16                                            [1, 256, 256]             789,248\n",
       "│    │    └─Dropout: 3-17                                                   [1, 256, 256]             --\n",
       "│    │    └─Linear: 3-18                                                    [1, 256, 512]             131,584\n",
       "│    └─LimbaBlock: 2-4                                                      [1, 256, 512]             --\n",
       "│    │    └─MambaBlock: 3-19                                                [1, 256, 512]             1,695,744\n",
       "│    │    └─Linear: 3-20                                                    [1, 256, 256]             131,328\n",
       "│    │    └─Dropout: 3-21                                                   [1, 256, 256]             --\n",
       "│    │    └─LinformerBlock: 3-22                                            [1, 256, 256]             789,248\n",
       "│    │    └─Dropout: 3-23                                                   [1, 256, 256]             --\n",
       "│    │    └─Linear: 3-24                                                    [1, 256, 512]             131,584\n",
       "│    └─LimbaBlock: 2-5                                                      [1, 256, 512]             --\n",
       "│    │    └─MambaBlock: 3-25                                                [1, 256, 512]             1,695,744\n",
       "│    │    └─Linear: 3-26                                                    [1, 256, 256]             131,328\n",
       "│    │    └─Dropout: 3-27                                                   [1, 256, 256]             --\n",
       "│    │    └─LinformerBlock: 3-28                                            [1, 256, 256]             789,248\n",
       "│    │    └─Dropout: 3-29                                                   [1, 256, 256]             --\n",
       "│    │    └─Linear: 3-30                                                    [1, 256, 512]             131,584\n",
       "│    └─LimbaBlock: 2-6                                                      [1, 256, 512]             --\n",
       "│    │    └─MambaBlock: 3-31                                                [1, 256, 512]             1,695,744\n",
       "│    │    └─Linear: 3-32                                                    [1, 256, 256]             131,328\n",
       "│    │    └─Dropout: 3-33                                                   [1, 256, 256]             --\n",
       "│    │    └─LinformerBlock: 3-34                                            [1, 256, 256]             789,248\n",
       "│    │    └─Dropout: 3-35                                                   [1, 256, 256]             --\n",
       "│    │    └─Linear: 3-36                                                    [1, 256, 512]             131,584\n",
       "├─LayerNorm: 1-3                                                            [1, 256, 512]             1,024\n",
       "├─Linear: 1-4                                                               [1, 256, 662]             339,606\n",
       "=============================================================================================================================\n",
       "Total params: 17,166,998\n",
       "Trainable params: 17,166,998\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 24.99\n",
       "=============================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 130.22\n",
       "Params size (MB): 68.25\n",
       "Estimated Total Size (MB): 198.47\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(1, 256),dtypes=[torch.long],device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3e1157-f0bc-4b23-bda5-022b82dea434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a0cef46-6631-42c1-8136-e76ecbc15df8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 269,\n",
       " 482,\n",
       " 395,\n",
       " 366,\n",
       " 7,\n",
       " 511,\n",
       " 7,\n",
       " 395,\n",
       " 379,\n",
       " 430,\n",
       " 7,\n",
       " 511,\n",
       " 7,\n",
       " 77,\n",
       " 307,\n",
       " 388,\n",
       " 10,\n",
       " 7,\n",
       " 500,\n",
       " 488,\n",
       " 487,\n",
       " 500,\n",
       " 357,\n",
       " 379,\n",
       " 379,\n",
       " 475,\n",
       " 10,\n",
       " 7,\n",
       " 381,\n",
       " 511,\n",
       " 7,\n",
       " 175,\n",
       " 426,\n",
       " 7,\n",
       " 522,\n",
       " 457,\n",
       " 52,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenized_dataset[1000]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3757d0b-2b41-47ac-93f2-d34d6f4dcf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "        self.pad_token_id = silabi_tokenizer.pad_token_id\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_dataset[idx]['input_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b16643c1-adb5-499d-9e37-45c89e8912cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_tokenized = TokenizedDataset(train_tokenized_dataset)\n",
    "test_tokenized = TokenizedDataset(test_tokenized_dataset)\n",
    "#val_tokenized = TokenizedDataset(val_tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "990b9609-2703-4afe-ac5d-f3444c33f7a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_tokenized_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m val_tokenized \u001b[38;5;241m=\u001b[39m TokenizedDataset(val_tokenized_dataset)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_tokenized_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "val_tokenized = TokenizedDataset(val_tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9df1d35a-7fd7-4e93-9c42-e8994f9b8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # Optional, for progress bar\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=10, learning_rate=1e-4, device='cuda'):\n",
    "    # Move model to the specified device (GPU or CPU)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize the loss function\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding token (assumed to be 0)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Loop through the training data\n",
    "        for batch_idx, input_ids in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "            input_ids = input_ids.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: get model output\n",
    "            output = model(input_ids)\n",
    "\n",
    "            # The output shape should be [batch_size, seq_len, vocab_size]\n",
    "            # For language modeling, the target is the input shifted by 1 position\n",
    "            target = input_ids[:, 1:].contiguous()  # Shift input for the target\n",
    "\n",
    "            # Pad the target to the same length as the output (if needed)\n",
    "            if target.size(1) < output.size(1):\n",
    "                target = F.pad(target, (0, output.size(1) - target.size(1)), value=0)\n",
    "\n",
    "            # Flatten the output and target tensors for CrossEntropyLoss\n",
    "            output = output.view(-1, output.size(-1))  # Flatten the output tensor\n",
    "            target = target.view(-1)  # Flatten the target tensor\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print the statistics for the current epoch\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "affe9e2d-a9f1-473a-a326-ed4e636fabe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_device = torch.device(\"mps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49156538-ae6c-4f31-84b9-25c62be72412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5fbb60b4-57c7-46d7-b751-e38b6ee091fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_tokenized,batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ff45a-30ec-4931-bf22-67c19d359aee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assuming train_loader and val_loader are your data loaders\n",
    "train_model(model, test_loader,num_epochs=3, device=\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3254c-404a-4bf7-b908-e0e2403eae51",
   "metadata": {},
   "source": [
    "## GPT-2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1f0866e6-6887-43f2-a286-7d27039929aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return silabi_tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa5b5a02-686d-4471-978b-0fefe60a6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = test_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8110bb87-a00a-40a0-a96b-8a4b170ab2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_tokenized_dataset = val_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "49eb1520-fda0-4e0f-a373-6b2d94f2a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7f181781-6dbf-4782-8e98-d0e47bf8cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "55dedc74-6f60-4c77-9c28-f53be11019bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=silabi_tokenizer,\n",
    "    mlm=False,  # GPT-2 is not trained with masked language modeling\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e71968c-27fb-4973-9ebd-341d47e3853d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e6cb71-6a5e-49f9-a421-2043454b88b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d278d01-bfd7-4603-af71-007668e69c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "43c26e27-647f-4101-9cb2-4c7b6d577741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model,GPT2Config, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7b6ec95d-573b-4899-898d-cf3b3f5ae07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=silabi_tokenizer.vocab_size,\n",
    "    n_positions=1024,\n",
    "    n_ctx=1024,\n",
    "    n_embd=768,\n",
    "    n_layer=12,\n",
    "    n_head=12,\n",
    "    bos_token_id=silabi_tokenizer.bos_token_id,\n",
    "    eos_token_id=silabi_tokenizer.eos_token_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "834cc6ea-84de-4cda-83c9-a5745da2d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "abce2557-2480-459e-963d-1bbb5988af72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "GPT2LMHeadModel                                    [1, 12, 256, 64]          --\n",
       "├─GPT2Model: 1-1                                   [1, 12, 256, 64]          --\n",
       "│    └─Embedding: 2-1                              [1, 256, 768]             508,416\n",
       "│    └─Embedding: 2-2                              [1, 256, 768]             786,432\n",
       "│    └─Dropout: 2-3                                [1, 256, 768]             --\n",
       "│    └─ModuleList: 2-4                             --                        --\n",
       "│    │    └─GPT2Block: 3-1                         [1, 256, 768]             7,087,872\n",
       "│    │    └─GPT2Block: 3-2                         [1, 256, 768]             7,087,872\n",
       "│    │    └─GPT2Block: 3-3                         [1, 256, 768]             7,087,872\n",
       "│    │    └─GPT2Block: 3-4                         [1, 256, 768]             7,087,872\n",
       "│    │    └─GPT2Block: 3-5                         [1, 256, 768]             7,087,872\n",
       "│    │    └─GPT2Block: 3-6                         [1, 256, 768]             7,087,872\n",
       "│    │    └─GPT2Block: 3-7                         [1, 256, 768]             7,087,872\n",
       "│    │    └─GPT2Block: 3-8                         [1, 256, 768]             7,087,872\n",
       "│    │    └─GPT2Block: 3-9                         [1, 256, 768]             7,087,872\n",
       "│    │    └─GPT2Block: 3-10                        [1, 256, 768]             7,087,872\n",
       "│    │    └─GPT2Block: 3-11                        [1, 256, 768]             7,087,872\n",
       "│    │    └─GPT2Block: 3-12                        [1, 256, 768]             7,087,872\n",
       "│    └─LayerNorm: 2-5                              [1, 256, 768]             1,536\n",
       "├─Linear: 1-2                                      [256, 662]                508,416\n",
       "====================================================================================================\n",
       "Total params: 86,859,264\n",
       "Trainable params: 86,859,264\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 163.40\n",
       "====================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 213.69\n",
       "Params size (MB): 347.44\n",
       "Estimated Total Size (MB): 561.13\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt2_model(test_tokenized[0])\n",
    "summary(gpt2_model, test_tokenized[0].shape,dtypes=[torch.long],device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2382eb84-2d65-44eb-9d9d-89b889fe1d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=gpt2_model,\n",
    "    args=training_args,\n",
    "    train_dataset=test_tokenized,\n",
    "    eval_dataset=val_tokenized_dataset,\n",
    "    processing_class=silabi_tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0c047798-e186-4dd9-83bc-235417160309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='156' max='432507' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   156/432507 02:01 < 94:36:49, 1.27 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2165\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2166\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2167\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2168\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2169\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2527\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2522\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2525\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2528\u001b[0m ):\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2530\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4fecf-b351-4ce2-a9c3-55c6b8171803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a7de97-e552-4b0d-babe-6e8af4fccf61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
